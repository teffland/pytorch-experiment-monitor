{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch monitor walkthrough\n",
    "\n",
    "This notebook serves as an example and test of `pytorch-monitor`.\n",
    "\n",
    "Pytorch is great. Dynamic computation and module composition make rapid prototyping of new deep learning architectures relatively simple. But as many machine learning engineers know, implementation time amounts to a small fraction of development relative to debugging and train/test evaluation.\n",
    "\n",
    "For new architectures, we often want to monitor various metrics during training and testing:\n",
    "* **Model parameter values:** Very large or skewed weight and bias distributions often are a sign of overfitting and small values can be due to underfitting or overregularization.\n",
    "* **Parameter gradient values:** Viewing gradient distributions for model parameters can shed light on vanishing and exploding gradients.  \n",
    "* **Model parameter update dynamics:** Understanding how model parameters change as a result of an optimization step can shed light on optimization issues. Consistently small parameter changes can indicate optimization is suffering from poor conditioning and large update values (especially late in training) often coincide with model performance divergences.\n",
    "* **Intermediate computation activations and gradients:** For deeper or more complicated architectures, it's often useful to monitor intermediate model activation (forward) and jacobian (backward) distributions. This can be crucial in identifying problematic computation nodes and exploding/vanishing gradients.\n",
    "* **Visualization of computation graph:** Visually inspecting the computation graph can be extremely useful in veryifying that your implementation plays out the same way as you drew it on the whiteboard.\n",
    "\n",
    "To monitor these metrics we need:\n",
    "1. Code that indicates the parameters, computations, and gradients to monitor within the model\n",
    "2. Code that writes summaries of these marked computation nodes efficiently to logfiles\n",
    "3. Code that provides an interface for inspecting the logs\n",
    "\n",
    "Tensorboard==1.6 and tensorboardX are great tools for (2) and (3), but the use of tensorboardX for writing summaries of all useful computations (as in (1)) can still be timeconsuming.\n",
    "\n",
    "This is where pytorch-monitor comes in. It's a simple library which aims to do two things:\n",
    "1. Initialize and organize experiment logs easily while encouraging reproducability\n",
    "2. Introspect initialized pytorch `Module`s and endow them with the necessary info to monitor themselves.\n",
    "\n",
    "Our goal is for this process to be fast with sensible defaults, but flexible enough to allow for changes that meet user needs. With pytorch-monitor you can be off the ground monitoring your network behavior in as little as 2 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_monitor import init_experiment, monitor_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: The Basics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Test Monitor',\n",
       " 'log_dir': 'test',\n",
       " 'run_name': 'Apr-30-18@22:56:49-toms-mbp-2.lan',\n",
       " 'run_dir': 'test/Apr-30-18@22:56:49-toms-mbp-2.lan',\n",
       " 'tag': 'Experiment Config: Test Monitor :: Apr-30-18@22:56:49\\n'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'title':'Test Monitor',\n",
    "          'log_dir':'test'}\n",
    "writer, config = init_experiment(config)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot just happened:\n",
    "\n",
    "We've taken the experiment config dictionary and does the following:\n",
    "1. Creates a `run_name`\n",
    "2. Creates a directory `run_name` in the path `log_dir`\n",
    "3. If you use git to track your code, automatically commit the curent git repository. A reproducible snapshot of the code used to run the experiment. \n",
    "4. Initializes a tensorboardX `SummaryWriter`\n",
    "5. Writes a tensorboard text log that logs all initial experiment metadata and configuration, such as start time, host name, random seed, and hyperparameters. This can be very useful reference when comparing multiple runs from tensorboard and drawing conclusions.\n",
    "6. If a `random_seed` is provided, it sets the seed for python, numpy, and torch all at once.\n",
    "7. Dumps the config dictionary to file in the `log_dir` as `config.json`\n",
    "8. Returns the writer and config dictionary, augmented with the `run_name`, `run_dir`, and `tag`.\n",
    "\n",
    "I've found this initialization strategy to be comprehensive enough for my needs, but I'm sure it could be better. Submit an issue or pull request if you have suggestions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a simple nonlinear 1-d regression in pytorch. It will take a scalar input x, run it through a nonlinear layer, and produce a scalar output.\n",
    "\n",
    "We'll train it to predict the sine function. $y = sin(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "\n",
    "# Declare the model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encode = nn.Linear(1, hidden_dim)\n",
    "        self.decode = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.tanh(self.encode(x))\n",
    "        self.monitor('h', h, track_data=True, track_grad=True)\n",
    "        return self.decode(h)\n",
    "\n",
    "# Get inputs and gold outputs\n",
    "x = 2 * 3.1415 * torch.rand(100,1)\n",
    "y = x.sin()\n",
    "\n",
    "# Setup the model and optimizer to have 10 hidden units\n",
    "model = Model(2)\n",
    "monitor_module(model, writer, track_update_ratio=True)\n",
    "optimizer  = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "_, (ax0, ax1) = plt.subplots(1,2, figsize=(12,4))\n",
    "# Train the model for 4001 steps, recording its predictions every 2000\n",
    "losses = []\n",
    "ax1.scatter(x.numpy(), y.numpy(), label='Truth', s=25, marker='^')\n",
    "for i in range(4001):\n",
    "    if i % 100 == 0:\n",
    "        model.monitoring(True)\n",
    "    else:\n",
    "        model.monitoring(False)\n",
    "    yhat = model(x)\n",
    "    loss = ((yhat - y)**2).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    writer.add_scalar('train loss', loss.item(), i)\n",
    "    \n",
    "    if i % 2000 == 0:\n",
    "        ax1.scatter(x.numpy(), yhat.detach().numpy(), label='Step {}'.format(i), s=20)\n",
    "\n",
    "ax0.plot(list(range(len(losses))), losses)\n",
    "ax0.set_title('Loss during training')\n",
    "ax1.set_title('Predictions During Training')\n",
    "_ = ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
